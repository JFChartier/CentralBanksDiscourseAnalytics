---
title: "DiscrepancyDiscourseAnalysis"
author: "Jean-Francois Chartier"
date: "12 f√©vrier 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#imports
```{r}
library(magrittr)
library(Matrix)
library(quanteda)
library(proxy)
library(ggplot2)
library(caret)
library(e1071)
```


#functions
```{r}
#function to project query in latent semantic space
  #'@param matrixV, a term-dim matrix m*k, of m terms previouslly modelleled with a SVD of k latent dimensions
  #'@param sigularValues, the k singular values of the train SVD
  #'@param newData, a new document-term matrix n*m to be projected in the latent space of k dimensions 
predictFromTrainSvdModel<-function(matrixV, singularValues, newData)
{
  call <- match.call()
  tsa <-  newData %*% matrixV %*% solve(diag((singularValues)))
  result <- list(docs_newspace = tsa)
  return (result)
}

#function to create a unit normed vector
normVector <- function(x) 
{
  if(sum(x)==0)
    return (x)
  else 
    return (x / sqrt(sum(x^2)))
  
}
#function to norm many vectors
normRowVectors<-function(m){
  t(apply(m, MARGIN = 1, FUN = function(x) normVector(x)))
}

#function to compute the dot product between 2 vectors
#note that when dot-product is applied to 2 unit vectors, it is the same as computing the cosine metric
dotProduct <- function(x, y) sum(x * y)
```




#load data
c'est long...
```{r}
myMatrix = readRDS("2Banks_sparseMatrix-2018-11-05.rds")
    
myData = readRDS("dataForDashboardWithoutRareWords-2018-11-05.rds")
myData[,c(9,10,11)]=NULL    
myReducedMatrix=readRDS("2Banks_approxReducedMatrix-2018-11-05.rds")
latentNormedDocSpace = as.matrix(myReducedMatrix$u %*% solve(diag((myReducedMatrix$d)))) %>% normRowVectors()

selected.segment.by.threshold=readRDS("selected.segment.by.threshold.rds")


```

#select threshold
```{r}
j=which(colnames(selected.segment.by.threshold)==0.2)
relevantId=selected.segment.by.threshold[,j]
myQueryData=myData[relevantId, ]
```

#Frequency distribution by bank
```{r}
boe.freq.year=myQueryData$year %>% subset(., myQueryData$bank=="BoE")%>%table(.)

ecb.freq.year=myQueryData$year %>% subset(., myQueryData$bank=="ECB")%>%table(.)

years=seq(from=1999, to=2018, by=1)
freq.by.year=data.frame(year=years, england=rep(0,length(years)), europe=rep(0,length(years)), stringsAsFactors = F)

for(i in 1: length(years)){
  #print(i)
  y=years[i]
  
  freq.by.year$england[i]=boe.freq.year[which(names(boe.freq.year)==y)] %>%ifelse(test = length(.)>0, yes = ., no = 0)
  freq.by.year$europe[i]=ecb.freq.year[which(names(ecb.freq.year)==y)]%>%ifelse(test = length(.)>0, yes = ., no = 0)
  
}

```

##plot frequency distribution by bank
```{r}
library(reshape2)
x=melt(freq.by.year, id.vars=1) %>%set_colnames(., c("year", "bank", "n.segment"))

ggplot(x, aes(x = year, y = n.segment, color = bank)) +
  theme_bw() + geom_line(size=2)+
  ylab("Number of segments")+xlab("Years")+
  #theme(axis.text.x = element_text(angle = 90))+
  labs(title = "Frequency distribution of semantically relevant segments")
```


#compute similarity between relevant segments
```{r}
relevant.vectors=latentNormedDocSpace[relevantId,]

simil.betw.bank=proxy::simil(x=relevant.vectors[myQueryData$bank=="BoE",], y = relevant.vectors[myQueryData$bank=="ECB",], by_rows=T, method=dotProduct, convert_distances = FALSE)

years=seq(from=1999, to=2018, by=1)
avg.sim.by.year=data.frame(year=integer(length(years)), avg.sim=double(length(years)), std=double(length(years)), stringsAsFactors = F)

for(i in 1: length(years)){
  year_i=years[i]
  
  x=myQueryData%>%subset(., bank=="BoE")%>%.$year==year_i
  y=myQueryData%>%subset(., bank=="ECB")%>%.$year==year_i
  
  sub.simil.year=simil.betw.bank[x, y]
  avg.sim.by.year$year[i]=year_i
  avg.sim.by.year$avg.sim[i]=mean(sub.simil.year)
  avg.sim.by.year$std[i]=sd(sub.simil.year)
  
}

```

##plot averaged similarity distribution
```{r}
#plot(avg.sim.by.year$avg.sim, type = "l")
p=ggplot(avg.sim.by.year, aes(x=year, y=avg.sim))+
  geom_line()+
  geom_errorbar(aes(ymin=avg.sim-std, ymax=avg.sim+std), colour="black", width=.1)+
  geom_point(size=3)

p+labs(title="Evolution of discourse discrepancy between Central Banks\n measered with the cosine metric", x="Year", y = "Averaged cosine", subtitle="error bar = standard deviation ")
```

#compute similarity between relevant segments
```{r}

years=seq(from=1999, to=2018, by=1)
avg.sim.by.year=data.frame(year=integer(length(years)), avg.sim=double(length(years)), std=double(length(years)), stringsAsFactors = F)

for(i in 1: length(years)){
  year_i=years[1]
  print(year_i)
  x=myData$bank=="BoE" & myData$year==year_i
  y=myData$bank=="ECB" & myData$year==year_i
  #x=myData%>%subset(., bank=="BoE")%>%.$year==year_i
  #y=myData%>%subset(., bank=="ECB")%>%.$year==year_i
  simil.betw.bank=proxy::simil(x=latentNormedDocSpace[x,], y = latentNormedDocSpace[y,], by_rows=T, method=dotProduct, convert_distances = FALSE)
  
  avg.sim.by.year$year[i]=year_i
  avg.sim.by.year$avg.sim[i]=mean(simil.betw.bank)
  avg.sim.by.year$std[i]=sd(simil.betw.bank)
  
}

```


#Get Specificities
```{r}
#select bank
bank="ECB" #"BoE"

docvars(myMatrix, "ECB")<-((myData$bank=="ECB") & relevantId)
subDFM<-dfm_subset(myMatrix, relevantId)

docvars(subDFM, "BoE")=subDFM@docvars$ECB==F
    
#calculer les specificites
specificites = quanteda::textstat_keyness(x=subDFM, target=subDFM@docvars$ECB==T, measure="chi2", sort=TRUE)
    
#filter NA
specificites=specificites[is.na(specificites$chi2)==F,]
    
```

#Plot
```{r}

p=0.05
specificites=specificites[specificites$p<p]

#set number of specificities to plot 
nSpec=30
#set min word count. Since we used the chi2, we should keep only words above 10
min_count=10
quanteda::textplot_keyness(x=specificites, n=nSpec, show_legend = T, color = c("darkblue", "darkred"), labelsize=2.5, min_count=min_count, labelcolor="black", margin = 0.2)+
  ggplot2::labs(title = "Lexical Specificities of Central Banks", x = "Chi-2", y = "Lexical specificities", color = "Central Banks")+theme(plot.margin = unit(c(1,4,1,2), "cm"))+ theme(legend.text = element_text(colour="black"))+
  scale_color_manual(labels = c("ECB", "BoE"), values = c("darkblue", "darkred"))

```

