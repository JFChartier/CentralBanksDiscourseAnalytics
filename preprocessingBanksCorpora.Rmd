---
title: "PreprocessingCorpora"
author: "Jean-Francois Chartier"
date: "Febuary 12 2019"
output: 
  html_document:
    fig_height: 12
    fig_width: 9
    number_sections: yes
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache.lazy = FALSE, cache=T, warning = FALSE, message = FALSE)
```

#Install packages
```{r, cache=T}
if ("quanteda" %in% installed.packages()==FALSE)
  {
    install.packages('quanteda',dependencies = TRUE)
  }

library(quanteda)
#fixe le nombre de processeurs utilises
quanteda::quanteda_options("threads" = 7)


if ("Matrix" %in% installed.packages()==FALSE)
  {
    install.packages('Matrix',dependencies = TRUE)
  }

library(Matrix)



if ("stringr" %in% installed.packages()==FALSE)
  {
    install.packages('stringr',dependencies = TRUE)
  }

library(stringr)
if ("irlba" %in% installed.packages()==FALSE)
{
  install.packages('irlba',dependencies = TRUE)
}
library(irlba)
if ("textstem" %in% installed.packages()==FALSE)
{
  install.packages('textstem',dependencies = TRUE)
}
library(textstem)



```

#Load Data
```{r ,cache=T}
myData=readRDS("df_BoE_AND_ECB_paragraphs.rds")
myMetaData=readRDS("df_BoE_AND_ECB_info.rds")
```

##sampling for testing
```{r}
#myData=myData[1:1000,]
#myMetaData=myMetaData[1:1000,]
```


#Select english document
```{r}
idEng =myMetaData$doc_ID[myMetaData$english==T]

idSeg = myData$doc_ID %in% idEng
myData=myData[idSeg,]
myMetaData=myMetaData[idSeg,]
```

##encode into utf8
```{r,cache=T}
for (i in 1:length(myData$rawText)) 
  Encoding(myData$rawText[i]) <- "UTF-8"

```



#Cleaning pdf
```{r ,cache=T}

#remove break line
preprocesCorpus=stringr::str_replace_all(myData$rawText,"[\r\n]" , "")

#remove hyphens
preprocesCorpus=stringr::str_replace_all(preprocesCorpus,"-", "")

#remove all non graphical caracther
preprocesCorpus=stringr::str_replace_all(preprocesCorpus,"[^[:graph:]]", " ")

#preprocesCorpus=stringr::str_replace_all(preprocesCorpus,"\t", " ")

preprocesCorpus=stringr::str_squish(preprocesCorpus)

```



#preprocessing 
```{r}

# tokenization with quanteda
preprocesCorpus=quanteda::tokens(x=preprocesCorpus,what="word", remove_punct = TRUE, remove_numbers = TRUE, remove_separators = TRUE,remove_hyphens = TRUE, remove_symbols=TRUE, remove_url = TRUE)

preprocesCorpus=quanteda::tokens_tolower(preprocesCorpus)

#myStopWords=unique(c(stopwords("en", source = "smart"), c("yes", "no", "thing", "can", "okay", "ok", "just", "good", "like", "something", "one", "moment", "say", "go", "speeches", "pages", "online", "default.aspx", "www.bankofengland.co.uk")))

myStopWords=unique(c(stopwords("en", source = "smart")))

# filtrer selon un antidictionnaire et singleton
preprocesCorpus=quanteda::tokens_remove(preprocesCorpus, case_insensitive = F, valuetype = "glob", pattern=myStopWords, min_nchar=2)

#lemmatization
preprocesCorpus=sapply(preprocesCorpus, FUN = function(seg)  paste0(textstem::lemmatize_words(seg), collapse = " "))
preprocesCorpus=quanteda::tokens(preprocesCorpus)

#apply again stopwords after lemmatization
preprocesCorpus = quanteda::tokens_remove(preprocesCorpus, case_insensitive = TRUE, valuetype = "glob", pattern=myStopWords)

print(c("corpus size after preprocessing : " , length(paste(unlist(preprocesCorpus)))))

print(c("vocabulary size after preprocessing : ", length(unique(paste(unlist(preprocesCorpus))) )))

```


##Extract ngrams of words
```{r,cache=T}
preprocesCorpus2=quanteda::tokens(preprocesCorpus, ngrams=1:2)
```

#Find and filter rare words
```{r}
m=quanteda::nfeat(dfm(x=preprocesCorpus2, tolower=FALSE))
wordDocFreq=quanteda::dfm(x=preprocesCorpus2, tolower=FALSE) %>% topfeatures(., n=m, decreasing = FALSE, scheme="docfreq")

wordsTooRare=wordDocFreq[wordDocFreq<200]

wordsTooFrequent=wordDocFreq[wordDocFreq>(length(preprocesCorpus2)*.66)]

wordsToFilter=c(wordsTooRare, wordsTooFrequent)

preprocesCorpus3=quanteda::tokens_remove(preprocesCorpus2, case_insensitive = F, valuetype = "glob", pattern=names(wordsToFilter))

```

#Filter short segments
```{r}

lengthOfSegment=lapply(preprocesCorpus3, FUN = function(x) length(x))
#n=sum(lengthOfSegment>4)
preprocesCorpus3=preprocesCorpus3[lengthOfSegment>4,]
myData=myData[lengthOfSegment>4,]
myMetaData=myMetaData[lengthOfSegment>4,]

```

#save preprocessed data
```{r}
myData$tokens=preprocesCorpus3
saveRDS(myData, "2Banks_preProText-2019-02-25.rds")
saveRDS(myMetaData, "2Banks_metadata-2019-02-25.rds")
```

#Modeling documents
```{r ,cache=T}
#Vectorize documents 
myMatrix = quanteda::dfm(x=preprocesCorpus3, tolower=FALSE)

saveRDS(myMatrix, "2Banks_sparseMatrix-2019-02-25.rds")

# imprimer nombre de dimensions de la matrice
#print(paste("nombre de mots differents apres filtrage base sur la frequence documentaire : ", length(myMatrix@Dimnames$features)))

```



#SVD 400 with irlba
approximate the SVD, but it is faster than computing an exact SVD
```{r}
#myMatrix=readRDS("2Banks_sparseMatrix-2019-02-12.rds")
dfm_weight(x=myMatrix, scheme="prop") %>%saveRDS(., file = "2Banks_weightedSparseMatrix-2019-02-25.rds")
set.seed(1)
readRDS("2Banks_weightedSparseMatrix-2019-02-25.rds") %>% irlba::irlba(., 400, tol=1e-5)%>%saveRDS(., file = "2Banks_approxReducedMatrix-2019-02-12.rds")

#load(file = "myNyTimeTfIdfMatrixStem.RData")
#set.seed(1)
#dfm_weight(x=myMatrix, scheme="prop") %>% irlba::irlba(., 400, tol=1e-5)%>%saveRDS(., file = "2Banks_approxReducedMatrix-2019-02-12.rds")

```


```{r}
#saveRDS(object = myReducedMatrix, file = "2Banks_approxReducedMatrix-2019-02-12.rds")
```



